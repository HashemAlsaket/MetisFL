import cloudpickle
import queue
import os

import pandas as pd
from pebble import ProcessPool

from projectmetis.python.logging.metis_logger import MetisLogger
from projectmetis.python.models.model_dataset import ModelDataset
from projectmetis.python.models.model_meta import ModelMeta
from projectmetis.python.learner.learner_controller_client import LearnerControllerClient
from projectmetis.python.learner.learner_evaluator import LearnerEvaluator
from projectmetis.python.learner.learner_trainer import LearnerTrainer
from projectmetis.proto import learner_pb2, model_pb2, metis_pb2


class Learner(object):
    """
    Any invocation to the public functions of the Learner instance need to be wrapped inside a process,
    since the body of every function is generating a new Neural Network registry/context.

    In order to be able to run the training/evaluation/prediction functions as independent
    processes, we needed to create a ModelOperations class factory, depending on the neural network engine
    being used. For instance, for Keras we define the `_keras_model_ops_factory()` that internally imports
    the KerasModelOps class and for PyTorch we follow the same design by importing the PyTorchModeOps
    inside the body of the `_pytorch_model_ops_factory()` function.

    Specifically, no ModelOps subclass should be imported in the global scope of the learner but rather
    within the local scope (i.e., namespace) of each neural network model operations factory function.
    """

    def __init__(self, learner_server_entity: metis_pb2.ServerEntity,
                 controller_server_entity: metis_pb2.ServerEntity, nn_engine, model_fp,
                 train_dataset_fp, train_dataset_recipe_pkl,
                 validation_dataset_fp="", validation_dataset_recipe_pkl="",
                 test_dataset_fp="", test_dataset_recipe_pkl="",
                 concurrent_tasks=1, learner_credentials_fp="/tmp/metis/learner/"):
        self._learner_server_entity = learner_server_entity
        self._controller_server_entity = controller_server_entity
        self._nn_engine = nn_engine
        self._model_fp = model_fp
        self._train_dataset = self._create_model_dataset(
            train_dataset_recipe_pkl, train_dataset_fp)
        self._default_dataset_class = self._train_dataset.__class__
        self._valid_dataset = self._create_model_dataset(
            validation_dataset_recipe_pkl, validation_dataset_fp)
        self._test_dataset = self._create_model_dataset(
            test_dataset_recipe_pkl, test_dataset_fp)

        # A single Process per training/evaluation/inference tasks.
        self._concurrent_tasks = concurrent_tasks
        # We use pebble.ProcessPool() and not the native concurrent.futures.ProcessPoolExecutor() so that
        # when a termination signal - SIGTERM is received we stop immediately the active task. This was
        # not possible with jobs/tasks submitted to the concurrent.futures.ProcessPoolExecutor()
        # because we had to wait for the active task to complete.
        self._training_tasks_pool, self._training_tasks_futures_q = \
            ProcessPool(max_workers=self._concurrent_tasks), \
            queue.Queue(maxsize=self._concurrent_tasks)
        self._evaluation_tasks_pool, self._evaluation_tasks_futures_q = \
            ProcessPool(max_workers=self._concurrent_tasks), \
            queue.Queue(maxsize=self._concurrent_tasks)
        self._inference_tasks_pool, self._inference_tasks_futures_q = \
            ProcessPool(max_workers=self._concurrent_tasks), \
            queue.Queue(maxsize=self._concurrent_tasks)

        # The `learner_id` param is generated by the controller with the join federation request
        # and it is used thereafter for every incoming/forwarding request.
        self.__learner_credentials_fp = learner_credentials_fp
        if not os.path.exists(self.__learner_credentials_fp):
            os.mkdir(self.__learner_credentials_fp)
        self.__learner_id = None
        self.__auth_token = None
        # TODO if we want to be more secure, we can dump an
        #  encrypted version of auth_token and learner_id
        self.__learner_id_fp = os.path.join(self.__learner_credentials_fp, "learner_id.txt")
        self.__auth_token_fp = os.path.join(self.__learner_credentials_fp, "auth_token.txt")

    def __getstate__(self):
        """
        Python needs to pickle the entire object, including its instance variables.
        Since one of these variables is the Pool object itself, the entire object cannot be pickled.
        We need to remove the Pool() variable from the object state in order to use the pool_task.
        See also: https://stackoverflow.com/questions/25382455
        """
        self_dict = self.__dict__.copy()
        del self_dict['_training_tasks_pool']
        del self_dict['_training_tasks_futures_q']
        del self_dict['_evaluation_tasks_pool']
        del self_dict['_evaluation_tasks_futures_q']
        del self_dict['_inference_tasks_pool']
        del self_dict['_inference_tasks_futures_q']
        return self_dict

    def _clear_tasks(self, future_tasks_q, graceful=False):
        if future_tasks_q.full():
            while not future_tasks_q.empty():
                if graceful:
                    # await for the underlying future to complete
                    future_tasks_q.get().result()
                else:
                    # non-blocking retrieval of AsyncResult from queue
                    future_tasks_q.get(block=False).cancel()

    def _create_model_dataset(self, dataset_recipe_pkl, dataset_fp):
        # TODO Move into utils?
        if dataset_recipe_pkl and dataset_fp:
            dataset_recipe_fn = cloudpickle.load(open(dataset_recipe_pkl, "rb"))
            dataset = dataset_recipe_fn(dataset_fp)
            assert isinstance(dataset, ModelDataset)
        else:
            dataset = self._default_dataset_class()
        return dataset

    def _mark_learning_task_completed(self, training_future):
        # If the returned future was completed successfully and was not cancelled,
        # meaning it did complete its running job, then notify the controller.
        if training_future.done() and not training_future.cancelled():
            print("Inside Learning Task: Marking Learning Task Completed", flush=True)
            training_future_result = training_future.result()
            training_logs = training_future_result[0]
            trained_model = training_future_result[1]
            print("Training Logs", training_logs, flush=True)
            controller_client = LearnerControllerClient(controller_server_entity=self._controller_server_entity)
            model_meta = ModelMeta()
            aux_metadata = ""
            controller_client.mark_task_completed(learner_id=self.__learner_id,
                                                  auth_token=self.__auth_token,
                                                  model_weights=trained_model,
                                                  model_meta=model_meta,
                                                  aux_metadata=aux_metadata)

    def _model_ops_factory(self, nn_engine):
        if nn_engine == "keras":
            return self._model_ops_factory_keras
        if nn_engine == "pytorch":
            return self._model_ops_factory_pytorch

    def _model_ops_factory_keras(self, *args, **kwargs):
        from projectmetis.python.models.keras.keras_model_ops import KerasModelOps
        model_ops = KerasModelOps(model_filepath=self._model_fp, *args, **kwargs)
        return model_ops

    def _model_ops_factory_pytorch(self, *args, **kwargs):
        from projectmetis.python.models.pytorch.pytorch_model_ops import PyTorchModelOps
        model_ops = PyTorchModelOps(model_filepath=self._model_fp, *args, **kwargs)
        return model_ops

    def _normalize_dictionary(self, d):
        # TODO Move into utils?
        # Normalize dictionary to a flatten table.
        normalized_d = pd.json_normalize(d, sep="_")
        normalized_d = normalized_d.astype(str)
        normalized_d = normalized_d.to_dict(orient="records")[0]
        return normalized_d

    def host_port_identifier(self):
        return "{}:{}".format(
            self._learner_server_entity.hostname,
            self._learner_server_entity.port)

    def join_federation(self):
        # TODO If I create a learner controller instance once (without channel initialization)
        #  then the program hangs!
        MetisLogger.info("Learner {} joining federation.".format(self.host_port_identifier()))
        controller_client = LearnerControllerClient(controller_server_entity=self._controller_server_entity)
        self.__learner_id, self.__auth_token, status = \
            controller_client.join_federation(self._learner_server_entity,
                                              self.__learner_id_fp,
                                              self.__auth_token_fp,
                                              self._train_dataset,
                                              self._test_dataset,
                                              self._valid_dataset)
        MetisLogger.info("Learner {} joined federation.".format(self.host_port_identifier()))
        return status

    def leave_federation(self):
        MetisLogger.info("Learner {} leaving federation.".format(self.host_port_identifier()))
        controller_client = LearnerControllerClient(controller_server_entity=self._controller_server_entity)
        status = controller_client.leave_federation(self.__learner_id, self.__auth_token)
        MetisLogger.info("Learner {} left federation.".format(self.host_port_identifier()))
        return status

    def model_evaluate(self, model_pb: model_pb2.Model, batch_size: int,
                       evaluation_datasets_pb: [learner_pb2.EvaluateModelRequest.dataset_to_eval],
                       metrics=None, verbose=False):
        MetisLogger.info("Learner {} starts model evaluation on requested datasets."
                         .format(self.host_port_identifier()))
        model_ops = self._model_ops_factory(self._nn_engine)
        learner_evaluator = LearnerEvaluator(model_ops)
        eval_res = {}
        for dataset_to_eval in evaluation_datasets_pb:
            if dataset_to_eval == learner_pb2.EvaluateModelRequest.dataset_to_eval.TRAINING:
                train_res = learner_evaluator.evaluate_model(self._train_dataset, model_pb,
                                                             batch_size, metrics, verbose)
                # TODO if dataset was empty and no evaluation occurred then assign NaN, maybe assign 0.0 instead?
                eval_res["train"] = train_res if train_res else float("nan")
            if dataset_to_eval == learner_pb2.EvaluateModelRequest.dataset_to_eval.TEST:
                test_res = learner_evaluator.evaluate_model(self._test_dataset, model_pb,
                                                            batch_size, metrics, verbose)
                # TODO if dataset was empty and no evaluation occurred then assign NaN, maybe assign 0.0 instead?
                eval_res["test"] = test_res if test_res else float("nan")
            if dataset_to_eval == learner_pb2.EvaluateModelRequest.dataset_to_eval.VALIDATION:
                val_res = learner_evaluator.evaluate_model(self._valid_dataset, model_pb,
                                                           batch_size, metrics, verbose)
                # TODO if dataset was empty and no evaluation occurred then assign NaN, maybe assign 0.0 instead?
                eval_res["valid"] = val_res if val_res else float("nan")
        normalized_res = self._normalize_dictionary(eval_res)
        MetisLogger.info("Learner {} completed model evaluation on requested datasets."
                         .format(self.host_port_identifier()))
        return normalized_res

    def model_infer(self, model_pb: model_pb2.Model, batch_size: int,
                    infer_train=False, infer_test=False, infer_valid=False, verbose=False):
        MetisLogger.info("Learner {} starts model inference on requested datasets."
                         .format(self.host_port_identifier()))
        # TODO infer model should behave similarly as the evaluate_model(), by looping over a
        #  similar learner_pb2.InferModelRequest.dataset_to_infer list.
        model_ops = self._model_ops_factory(self._nn_engine)
        learner_evaluator = LearnerEvaluator(model_ops)
        inferred_res = {"train": {}, "test": {}, "valid": {}}
        if infer_train:
            train_inferred = learner_evaluator.infer_model(self._train_dataset, model_pb, batch_size, verbose)
            inferred_res["train"] = train_inferred
        if infer_test:
            test_inferred = learner_evaluator.infer_model(self._valid_dataset, model_pb, batch_size, verbose)
            inferred_res["valid"] = test_inferred
        if infer_valid:
            val_inferred = learner_evaluator.infer_model(self._test_dataset, model_pb, batch_size, verbose)
            inferred_res["valid"] = val_inferred
        normalized_res = self._normalize_dictionary(inferred_res)
        MetisLogger.info("Learner {} completed model inference on requested datasets."
                         .format(self.host_port_identifier()))
        return normalized_res

    def model_train(self, learning_task_pb: metis_pb2.LearningTask,
                    hyperparameters_pb: metis_pb2.Hyperparameters, model_pb: model_pb2.Model,
                    verbose=False):
        MetisLogger.info("Learner {} starts model training on local training dataset."
                         .format(self.host_port_identifier()))
        model_ops_fn = self._model_ops_factory(self._nn_engine)
        learner_trainer = LearnerTrainer(model_ops_fn)
        training_logs, trained_model = learner_trainer.train_model(self._train_dataset, learning_task_pb,
                                                                   hyperparameters_pb, model_pb, verbose)
        normalized_logs = self._normalize_dictionary(training_logs)
        MetisLogger.info("Learner {} completed model training on local training dataset."
                         .format(self.host_port_identifier()))
        return normalized_logs, trained_model

    def run_evaluation_task(self, model_pb: model_pb2.Model, batch_size: int,
                            evaluation_dataset_pb: [learner_pb2.EvaluateModelRequest.dataset_to_eval],
                            metrics: [str], verbose=False, block=False):
        # TODO No callback function when evaluation result is returned. Maybe we need to implement one more gRPC
        #  endpoint to the controller to retrieve latest model evaluation results? Similar to mark task completed?
        self._clear_tasks(future_tasks_q=self._evaluation_tasks_futures_q)
        # If we submit the datasets and the metrics as is (i.e., as repeated fields) pickle cannot
        # serialize the repeated messages and it requires converting the repeated messages into a list.
        evaluation_datasets_pb = [d for d in evaluation_dataset_pb]
        metrics = [m for m in metrics]
        future = self._evaluation_tasks_pool.schedule(
            function=self.model_evaluate, args=(model_pb, batch_size, evaluation_datasets_pb, metrics, verbose))
        self._evaluation_tasks_futures_q.put(future)
        # TODO Maybe return another result instead of this naive empty dictionary?
        eval_res = dict()
        if block:
            eval_res = future.result()
        return eval_res

    def run_inference_task(self):
        raise NotImplementedError("Not yet implemented.")

    def run_learning_task(self, learning_task_pb: metis_pb2.LearningTask,
                          hyperparameters_pb: metis_pb2.Hyperparameters, model_pb: model_pb2.Model,
                          verbose=False, block=False):
        # Always clear most recent tasks from the queue - process the last submitted task.
        self._clear_tasks(future_tasks_q=self._training_tasks_futures_q)
        # Submit the learning/training task to the Process Pool and add a callback to send the
        # trained local model to the controller when the learning task is complete. Given that
        # local training could span from seconds to hours, we cannot keep the grpc connection
        # open indefinitely and therefore the callback will collect the training result and
        # forward it accordingly to the controller.
        future = self._training_tasks_pool.schedule(
            function=self.model_train, args=(learning_task_pb, hyperparameters_pb, model_pb, verbose))
        # The following callback will trigger the request to the controller to receive the next task.
        future.add_done_callback(self._mark_learning_task_completed)
        self._training_tasks_futures_q.put(future)
        if block:
            future.result()
        # If the task is submitted for processing then it is not cancelled.
        is_task_submitted = not future.cancelled()
        return is_task_submitted

    def shutdown(self, graceful=False):
        # If graceful is True, it will allow all pending tasks to be completed,
        # else it will stop immediately all active tasks.
        self._clear_tasks(future_tasks_q=self._training_tasks_futures_q, graceful=graceful)
        self._training_tasks_pool.close()
        self._training_tasks_pool.join()
        self._clear_tasks(future_tasks_q=self._evaluation_tasks_futures_q, graceful=graceful)
        self._evaluation_tasks_pool.close()
        self._evaluation_tasks_pool.join()
        self._clear_tasks(future_tasks_q=self._inference_tasks_futures_q, graceful=graceful)
        self._inference_tasks_pool.close()
        self._inference_tasks_pool.join()
        # TODO - we always return True, but we need to capture any failures that may occur while terminating.
        return True
