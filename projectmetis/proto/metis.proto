syntax = "proto3";
package projectmetis;

import "projectmetis/proto/model.proto";

/////////////
// Generic //
/////////////

// A server entity that participates in the collaborative learning environment.
message ServerEntity {
  // Either the hostname or the IP address of the server.
  string hostname = 1;

  // The server port that the service is running on.
  uint32 port = 2;
}

//////////////
// Learning //
//////////////

// Dataset specification.
message DatasetSpec {
  uint32 num_training_examples = 1;
  uint32 num_validation_examples = 2;
  uint32 num_test_examples = 3;

  message ClassificationDatasetSpec {
    map<uint32, uint32> class_examples_num = 1;
  }

  message RegressionDatasetSpec {
    // TODO(canast02): Need to add histogram support; the following statistics
    //                 should refer to a histogram bucket.
    double min = 1;
    double max = 2;
    double mean = 3;
    double median = 4;
    double mode = 5;
    double stddev = 6;
  }

  oneof training_dataset_spec {
    ClassificationDatasetSpec training_classification_spec = 4;
    RegressionDatasetSpec training_regression_spec = 5;
  }
  oneof validation_dataset_spec {
    ClassificationDatasetSpec validation_classification_spec = 6;
    RegressionDatasetSpec validation_regression_spec = 7;
  }
  oneof test_dataset_spec {
    ClassificationDatasetSpec test_classification_spec = 8;
    RegressionDatasetSpec test_regression_spec = 9;
  }
}

message LearningTask {
  // This reflects the number of local steps the learner needs to perform.
  // It is similar to epochs if we take |num_training_examples| / batch_size.
  uint32 num_local_updates = 1;

  float training_dataset_percentage_for_stratified_validation = 2;

  // TODO: We need to define the metrics we expect the learner to return. This
  //  should be similar to the evaluation field in message `learner.EvaluateModelRequest`.
}

message CompletedLearningTask {
  // This is the model trained by the learner locally.
  Model model = 1;

  // These are the basic metadata sent by the learner to the controller whenever
  // a locally assigned training task is complete.
  TaskExecutionMetadata execution_metadata = 2;

  // These are additional metadata sent by the learner to the controller.
  // TODO No structured response yet, but in a future release this should follow
  //  a specific format.
  string aux_metadata = 3;
}

message TaskExecutionMetadata {
  // TODO: For every training, validation, and test score below, we need to
  //  define the metrics we expect the learner to return. These metrics should
  //  be the ones defined in the evaluation field of message `common.LearningTask.evaluation`.

  // A list with all training evaluations across all epochs.
  repeated EpochEvaluation training_scores = 1;

  // A list with all validation evaluations across all epochs.
  repeated EpochEvaluation validation_scores = 2;

  // A list of all test evaluations across all epochs.
  repeated EpochEvaluation test_scores = 3;

  // Learner may perform partial epochs, thus the float data type.
  float completed_epochs = 4;

  uint32 completed_batches = 5;

  uint32 batch_size = 6;

  // Time-per-epoch in milliseconds.
  float processing_ms_per_epoch = 7;

  // Time-per-batch in milliseconds.
  float processing_ms_per_batch = 8;
}

message EpochEvaluation {
  // The id of the epoch. This is an incremental value, i.e., serial number.
  // A learner is training continuously and therefore it can increment its epoch
  // id as it progresses its training.
  uint32 epoch_id = 1;

  // The associated score of the epoch.
  // This could be any numeric value, such as accuracy and f1-score in
  // classification and MSE in regression tasks.
  float epoch_score = 2;

  // Loss value of each evaluation.
  float epoch_loss = 3;
}

message ModelEvaluation {
  // Need to return the key-value formatted value FOR NOW! That is for every
  // string metric passed through the EvaluateModelRequest endpoint we reply a
  // <metric, value> collection.
  map<string, string> metric_values = 1;

  // TODO: Not sure if we need a simple json response or scores! For instance,
  //       we might need additional metrics such as confusion matrices...
}

// Wrapper for multiple model evaluations.
message ModelEvaluations {
  repeated ModelEvaluation evaluation = 1;
}

message Hyperparameters {
  uint32 batch_size = 1;
  OptimizerConfig optimizer = 2;
}

////////////////
// Controller //
////////////////

message ControllerParams {
  ServerEntity server_entity = 1;

  GlobalModelSpecs global_model_specs = 2;

  CommunicationSpecs communication_specs = 3;

  // TODO Remove this, we do not need them anymore here! Driver Logic!
  int32 federated_execution_cutoff_mins = 4;

  float federated_execution_cutoff_score = 5;

  message ModelHyperparams {
    // TODO Shall we replace (batch_size, optimizer) with Hyperparameters message?
    uint32 batch_size = 1;
    uint32 epochs = 2;
    OptimizerConfig optimizer = 3;
    // TODO Need to figure out, if this quantity will be used as part of the aggregation scheme or as part of the actual validation evaluation.
    float percent_validation = 4;
  }

  ModelHyperparams model_hyperparams = 6;
}

message GlobalModelSpecs {
  enum AggregationRule {
    UNKNOWN = 0;
    FED_AVG = 1;
  }

  AggregationRule aggregation_rule = 1;

  float learners_participation_ratio = 2;
}

message CommunicationSpecs {
  enum Protocol {
    UNKNOWN = 0;
    SYNCHRONOUS = 1;
    ASYNCHRONOUS = 2;
    // TODO: add support for more protocols, e.g., semi-sync
  }

  Protocol protocol = 1;
}

message LearnerDescriptor {
  string id = 1;
  string auth_token = 2;
  ServerEntity service_spec = 3;
  DatasetSpec dataset_spec = 4;
}

message LearnerState {
  // Describes the learner. It also includes the generated authorization token
  // for the learner.
  LearnerDescriptor learner = 1;

  // Learner's model lineage.
  repeated Model model = 3;
}
